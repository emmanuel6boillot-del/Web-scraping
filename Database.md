# Guide SQLite ‚Äì Base de donn√©es pour le projet de Web Scraping

## Objectif g√©n√©ral
Ce document explique **comment utiliser une base de donn√©es SQLite en Python** pour stocker les donn√©es scrap√©es **au fur et √† mesure du scraping**.

---

## I - Pourquoi utiliser une base de donn√©es dans ce projet ?

Dans un projet de scraping r√©el, on ne stocke pas les donn√©es :
- uniquement en m√©moire
- uniquement dans des fichiers CSV/JSON

On utilise une **base de donn√©es** pour :

- sauvegarder les donn√©es progressivement
- √©viter les pertes en cas d‚Äôerreur
- structurer proprement l‚Äôinformation
- permettre l‚Äôanalyse par un autre membre de l‚Äô√©quipe

üëâ Ici, nous utilisons **SQLite**, une base simple et professionnelle et surtout ... open-source ^^

---

## II - Qu‚Äôest-ce que SQLite ?

SQLite est une **base de donn√©es relationnelle** stock√©e dans **un simple fichier `.db`**.

Caract√©ristiques :
- pas de serveur
- pas de configuration
- int√©gr√©e √† Python
- portable

Exemple de fichier :
```

database.db

````

---

## III - Voir la base de donn√©es : DB Browser for SQLite

### Pourquoi ?
Sans outil graphique, une base SQLite est illisible.
Pour comprendre ce qui se passe, **vous devez installer un visualiseur**.

### DB Browser for SQLite

Lien officiel :  
https://sqlitebrowser.org/

---

### Installation
1. T√©l√©charger la version correspondant √† votre syst√®me
2. Installer le logiciel
3. Lancer DB Browser for SQLite

---

### Ouvrir la base du projet (quand vous l'aurez g√©n√©r√©)
1. Cliquer sur **Ouvrir une base de donn√©e**
2. S√©lectionner le fichier `database.db`
3. Aller dans l‚Äôonglet **Parcourir les donn√©es**

‚û°Ô∏è Vous verrez les lignes s‚Äôajouter pendant le scraping

‚ö†Ô∏è Ne pas modifier la base manuellement pendant que Python √©crit dedans

---

## IV - M√©thode du projet

> **La base de donn√©es est aliment√©e pendant le scraping**,  
> pas √† la fin.

Chaque annonce scrap√©e est :
- nettoy√©e
- imm√©diatement ins√©r√©e dans SQLite

‚û°Ô∏è La base devient la **source de v√©rit√© du projet**

---

## V - Architecture impos√©e du code

### R√®gle absolue
‚ùå Le scraper **ne fait jamais de SQL**  
‚úÖ Le SQL est **centralis√© dans une classe d√©di√©e**

---

### R√¥le de la classe `DatabaseManager`

La classe g√®re :
- la connexion SQLite
- la cr√©ation des tables
- l‚Äôinsertion des annonces
- la lecture des donn√©es
- la fermeture propre de la base

üëâ Le scraping **utilise la classe**, sans conna√Ætre SQLite.

---

## VI - Connexion √† la base (une seule fois)

```python
import sqlite3

conn = sqlite3.connect("database.db")
cursor = conn.cursor()
````

üìå Cette connexion est ouverte :

* une seule fois
* au d√©but du programme

---

## VII - Utiliser une classe pour g√©rer la base

### Pourquoi une classe ?

Sans classe :

* duplication de code
* m√©lange scraping / base de donn√©es
* erreurs fr√©quentes

Avec une classe :

* code clair
* responsabilit√©s s√©par√©es
* maintenance facile

---

### Cr√©ation de l‚Äôobjet base de donn√©es

```python
db = DatabaseManager("database.db")
```

Cette ligne :

* ouvre la base
* cr√©e la table si n√©cessaire
* pr√©pare la base pour le scraping

---

## VIII - Structure de la table `annonces`
On prend ici l'exemple d'une table contenant des annonces immobil√®res. 

```sql
annonces (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    titre TEXT NOT NULL,
    prix INTEGER NOT NULL,
    surface INTEGER,
    ville TEXT NOT NULL,
    nb_pieces INTEGER,
    date_scraping TEXT NOT NULL
)
```

---

## IX - Insertion imm√©diate pendant le scraping

### C√¥t√© scraping (simple)

```python
db.insert_annonce(
    titre,
    prix,
    surface,
    ville,
    nb_pieces
)
```

Le scraper :

* collecte les donn√©es
* les nettoie
* les envoie √† la base

---

### C√¥t√© base de donn√©e (cach√©)

* ajout automatique de la date
* requ√™te SQL s√©curis√©e
* commit imm√©diat
* gestion des erreurs

---

## X - Commit et persistance

```python
conn.commit()
```

üìå Sans `commit()` :

* rien n‚Äôest enregistr√©
* la base reste vide

Dans ce projet :

* commit apr√®s chaque annonce
* ou apr√®s chaque page

---

## XI - G√©rer les doublons

### Probl√®me

Une annonce peut appara√Ætre plusieurs fois.

### Solution

* champ `UNIQUE`
* gestion des erreurs SQLite

```sql
url TEXT UNIQUE
```

```python
except sqlite3.IntegrityError:
    pass
```

---

## XII - Voir la base se remplir (exp√©rience conseill√©e)

1. Ouvrir `database.db` dans DB Browser
2. Lancer le scraping
3. Cliquer sur **Refresh**

‚û°Ô∏è Les annonces apparaissent progressivement

---

## XIII - Lire les donn√©es depuis Python

```python
db.get_all_annonces()
```

```python
db.get_annonces_by_ville("Paris")
```

---

## XIV - Fermeture propre de la base

```python
db.close()
```

‚ö†Ô∏è Obligatoire en fin de script

Pour votre rapport faite des captures d'√©crans de DB Browser (SQLite)
